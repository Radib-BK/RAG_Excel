# ðŸ¤– RAG Chatbot - Complete Document Q&A System

A powerful Retrieval-Augmented Generation (RAG) chatbot that can process multiple document types, extract text using OCR, and answer questions using locally hosted AI models. Optimized for RTX 2060 GPUs but works on any hardware.

![RAG Chatbot Demo](screenshot/rag-page.png)

## âœ¨ Key Features

- ðŸ“„ **Multi-format Support**: PDF, DOCX, TXT, CSV, SQLite DB, JPG, PNG
- ðŸ” **OCR Text Extraction**: Extract text from images using Tesseract
- ðŸ§  **Semantic Chunking**: LangChain-style recursive text splitting preserves document structure
- ðŸ“Š **Enhanced Table Processing**: Auto-detection and structured extraction of tables from PDFs
- ðŸŽ¯ **Accurate Tokenization**: Real tokenizer integration for precise token counting
- ðŸ§  **Local AI Models**: TinyLlama LLM + SentenceTransformers embeddings
- âš¡ **GPU Optimized**: RTX 2060 ready with 4-bit quantization
- ðŸŒ **Web Interface**: FastAPI backend + Streamlit frontend
- ðŸ³ **Docker Ready**: Simple containerized deployment
- ðŸ’¾ **Persistent Storage**: FAISS vector database with rich metadata

## ðŸš€ Quick Start Guide

### **ðŸ¤” Which Method Should I Use?**

| Method | Best For | Performance | Setup Complexity |
|--------|----------|-------------|------------------|
| **Local Development** | Development, RTX 2060 users | âš¡ Excellent (GPU) | ðŸŸ¡ Medium |
| **Docker** | Production, Sharing, Deployment | ðŸ”„ Good (CPU) | ðŸŸ¢ Easy |

### **Method 1: Local Development (Recommended for GPU Users)**

#### **Step 1: Clone and Setup**
```cmd
# Clone the repository
git clone https://github.com/Radib-BK/RAG_Excel.git
cd RAG_Excel

# Create virtual environment
python -m venv .venv

# Activate virtual environment
.venv\Scripts\activate
```

#### **Step 2: Install Dependencies**

**For GPU Users (RTX 2060/RTX series):**
```cmd
# Install PyTorch with CUDA support first (REQUIRED for GPU)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install other requirements
pip install -r requirements.txt
```

**For CPU-Only Users:**
```cmd
# Install all requirements (PyTorch CPU version included)
pip install -r requirements.txt
```

**Setup Models & Directories:**
```cmd
# Run installation script (downloads models, creates directories)
python install.py
```

#### **Step 3: Install Tesseract OCR**
- **Windows**: Download from [Tesseract GitHub](https://github.com/UB-Mannheim/tesseract/wiki)
- **Add to PATH**: `C:\Program Files\Tesseract-OCR`

#### **Step 4: Start the Application**

**Option A: Automatic Launcher (Recommended):**
```cmd
cd RAG_Excel
.venv\Scripts\activate
python run.py
```
*This automatically starts both FastAPI backend and Streamlit frontend*

**Option B: Manual (2 Terminals):**

**Terminal 1 (FastAPI Backend):**
```cmd
cd RAG_Excel
.venv\Scripts\activate
python main.py
```
*Keep this terminal running - this starts the API server*

**Terminal 2 (Web Interface):**
```cmd
cd RAG_Excel
.venv\Scripts\activate
streamlit run streamlit_app.py
```
*Open a new terminal window for this*

**Access URLs:**
- API: http://localhost:8000
- Web UI: http://localhost:8501
- API Docs: http://localhost:8000/docs

### **Method 2: Docker Deployment**

**Prerequisites for Docker:**
```cmd
# Create .env file with your HuggingFace token
echo HUGGINGFACE_TOKEN=your_token_here > .env
```

#### **Option A: Quick Start (Recommended)**
```cmd
# API Only
docker-compose up --build rag-excel

# API + Web Interface
docker-compose --profile frontend up --build
```

#### **Option B: Manual Docker Build**
```cmd
# Build the Docker image
docker build -t rag-excel .

# Run API only
docker run -p 8000:8000 \
  -v ${PWD}/vector_store:/app/vector_store \
  -v ${PWD}/sample_files:/app/sample_files \
  --env-file .env \
  rag-excel

# Run with Streamlit frontend (requires 2 containers)
# Terminal 1 - API
docker run -p 8000:8000 --name rag-api \
  -v ${PWD}/vector_store:/app/vector_store \
  -v ${PWD}/sample_files:/app/sample_files \
  --env-file .env \
  rag-excel

# Terminal 2 - Streamlit
docker run -p 8501:8501 --link rag-api \
  -e API_BASE_URL=http://rag-api:8000 \
  rag-excel streamlit run streamlit_app.py --server.address 0.0.0.0
```

#### **Access URLs (Docker):**
- **API**: http://localhost:8000
- **Web UI**: http://localhost:8501 (if using frontend profile)
- **API Docs**: http://localhost:8000/docs

## ðŸ§ª Test Your Installation

**Terminal 3 (Testing):**
```cmd
cd RAG_Excel
.venv\Scripts\activate
# Make sure main.py is running in Terminal 1, then:
python test_api.py

# Test enhanced document processing features:
python test_enhanced_ingestion.py
```

Expected output:
```
ðŸ§ª Starting RAG Chatbot API Tests
âœ… API is healthy
âœ… Upload successful - Chunks created: 2
âœ… Query successful - Answer: This is a Retrieval-Augmented Generation...
ðŸŽ‰ All tests completed!

ðŸš€ Enhanced RAG Ingestion Test Suite
âœ… Created semantic chunks with enhanced metadata
ðŸ“Š Table detection and structured extraction working
ðŸŽ‰ All enhanced features validated!
```

## ðŸ“– How to Use

### **1. Upload Documents**
- **Web Interface**: Drag & drop files in Streamlit
- **API**: `POST /upload` with file attachment
- **Supported**: PDF, DOCX, TXT, CSV, DB, JPG, PNG

### **2. Ask Questions**
- **Web Interface**: Type questions in the text box
- **API**: `POST /query` with JSON `{"question": "your question"}`

### **3. Get Intelligent Answers**
The system will:
1. Find relevant content in your documents
2. Generate contextual answers using AI
3. Provide source attribution and confidence scores

## ðŸ”§ API Reference

### **Core Endpoints**

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | System health status |
| `/upload` | POST | Upload documents |
| `/query` | POST | Ask questions |
| `/stats` | GET | Vector store statistics |
| `/clear` | DELETE | Clear all documents |

### **Example Usage**

**Upload a document:**
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@document.pdf"
```

**Ask a question:**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the main points?"}'
```

## âš™ï¸ Configuration

### **Environment Variables**
Create `.env` file for custom settings:
```env
# AI Models (Get your free token from https://huggingface.co/settings/tokens)
HUGGINGFACE_TOKEN="hf_your_token_here"
MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Processing Settings
CHUNK_SIZE=512
CHUNK_OVERLAP=100
TOP_K_RESULTS=5
MAX_TOKENS=150

# Hardware
DEVICE=cuda  # or 'cpu' for CPU-only
VECTOR_STORE_PATH=./vector_store
```

### **Hardware Requirements**

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **RAM** | 8GB | 16GB+ |
| **GPU** | Any CUDA GPU | RTX 2060+ |
| **Storage** | 5GB | 10GB+ |
| **Python** | 3.9+ | 3.12+ |

## ðŸ³ Docker Information

The project includes a simplified Docker setup:

### **What's Included:**
- **Single Dockerfile**: CPU-optimized for reliable deployment
- **docker-compose.yml**: Multi-service orchestration
- **Production ready**: Python 3.12, proper health checks

### **Why CPU Docker?**
- âœ… **Simple deployment** - works on any server
- âœ… **No GPU dependencies** - fewer complications
- âœ… **Cost effective** - cheaper cloud hosting
- âœ… **Reliable** - consistent performance

### **For GPU Performance:**
Use local development setup with your RTX 2060 for best performance during development, then deploy with Docker for production.

## ðŸ› ï¸ Troubleshooting

### **Common Issues & Solutions**

**âŒ "Cannot connect to API"**
```cmd
# Check if server is running
curl http://localhost:8000/health
```

**âŒ "Tesseract not found"**
```cmd
# Windows: Add to PATH
set PATH=%PATH%;C:\Program Files\Tesseract-OCR
```

**âŒ "CUDA out of memory"**
```env
# Use CPU fallback in .env
DEVICE=cpu
```

**âŒ "PyTorch not using GPU"**
```cmd
# Reinstall PyTorch with CUDA (REQUIRED for GPU)
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Verify GPU detection
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

**âŒ "Model download fails"**
```cmd
# Check internet connection and retry
python install.py
```

**âŒ "Slow performance"**
- Use GPU: Set `DEVICE=cuda`
- Check GPU usage: `nvidia-smi`
- Reduce chunk size: `CHUNK_SIZE=256`

## ðŸ“ Project Structure

```
RAG_Excel/
â”œâ”€â”€ main.py                      # FastAPI backend server
â”œâ”€â”€ streamlit_app.py            # Web interface
â”œâ”€â”€ run.py                      # Automatic launcher (starts both backend & frontend)
â”œâ”€â”€ ingestion.py                # Enhanced document processing with semantic chunking
â”œâ”€â”€ embeddings.py               # Vector embeddings & FAISS
â”œâ”€â”€ query.py                    # RAG engine & LLM
â”œâ”€â”€ utils.py                    # OCR utilities
â”œâ”€â”€ config.py                   # Configuration management
â”œâ”€â”€ install.py                  # Automated installation script
â”œâ”€â”€ test_api.py                 # API test suite
â”œâ”€â”€ test_enhanced_ingestion.py  # Enhanced processing test suite
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Docker configuration
â”œâ”€â”€ docker-compose.yml          # Multi-service setup
â”œâ”€â”€ README.md                   # This guide
â””â”€â”€ ENHANCED_FEATURES.md        # New features documentation
```


## ðŸ”„ Advanced Usage

### **Enhanced Document Processing**
The system now features **semantic chunking** that preserves document structure:
```python
# Automatically detects and preserves:
# - Paragraph boundaries
# - Table structures  
# - Page numbers
# - Content types (text, tables, figures)
```

### **Table-Aware Querying**
Ask specific questions about tables in your documents:
```
"What data is in the sales table?"
"Show me the pricing information from the product table"
```

### **Multiple Documents**
Upload multiple files - the system will search across all documents:
```python
# The system automatically combines context from multiple sources
```

### **Image OCR**
Upload images with text - OCR will extract and make it searchable:
```python
# Supports JPG, PNG with automatic text extraction
```

### **Custom Models**
Modify `config.py` to use different AI models:
```python
# Try different embedding models for better accuracy
# Or different LLMs for various response styles
```

---

## ðŸŽ‰ Quick Summary - Start to Finish

### **ðŸ–¥ï¸ Local Development (Best Performance):**

1. **Clone & Setup Environment:**
   ```cmd
   git clone https://github.com/Radib-BK/RAG_Excel.git
   cd RAG_Excel
   python -m venv .venv
   .venv\Scripts\activate
   ```

2. **Install Dependencies:**
   
   **For GPU (RTX 2060+):**
   ```cmd
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   pip install -r requirements.txt
   python install.py
   ```
   
   **For CPU Only:**
   ```cmd
   pip install -r requirements.txt
   python install.py
   ```

3. **Install Tesseract OCR:**
   - Download from [Tesseract GitHub](https://github.com/UB-Mannheim/tesseract/wiki)
   - Add `C:\Program Files\Tesseract-OCR` to PATH

4. **Start the Application:**
   ```cmd
   python run.py
   ```

5. **Access:** http://localhost:8501

### **ðŸ³ Docker Deployment (Easy Setup):**

1. **Clone & Setup:**
   ```cmd
   git clone https://github.com/Radib-BK/RAG_Excel.git
   cd RAG_Excel
   echo HUGGINGFACE_TOKEN=your_token_here > .env
   ```

2. **Run with Docker:**
   ```cmd
   # API + Web Interface
   docker-compose --profile frontend up --build
   
   # OR API Only
   docker-compose up --build rag-excel
   ```

3. **Access:** http://localhost:8501 (frontend) or http://localhost:8000 (API only)

### **ðŸ§ª Test Installation:**
```cmd
# In a new terminal
python test_api.py
```

**Start by running `python run.py` and then access http://localhost:8501 to upload your first document!** ðŸš€

